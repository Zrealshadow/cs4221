{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS,ASYNCHRONOUS\n",
    "from influxdb_client.client.query_api import TableList\n",
    "from influxdb_client.client.write_api import Point\n",
    "import s2cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "token = \"267rK84Nww4rPYmhqRyPAVXgOsZSyL07kzbexhKGMf6nRMHfOfa6KNEMxCVX08QWQvxzBZRVU0DWJazUEoNolg==\"\n",
    "# export your token into the environment variable INFLUXDB_TOKEN first\n",
    "url = \"http://10.10.10.247:8086\"\n",
    "# replace url with your server address\n",
    "# if your server is on the same machine, use \"http://localhost:8086\"\n",
    "\n",
    "# set time 30s, the default is 10s may not be enough in this tutorial\n",
    "timeout = 60 * 1000\n",
    "client = influxdb_client.InfluxDBClient(url=url, token=token, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutoriakl we will investigate the data from Hartford Police Department\n",
    "# This historical dataset reflects reported incidents of crime (with the execption of sexual assaults)\n",
    "# that occurred in the City of Hartford from January 1, 2005 to May 18, 2021.\n",
    "\n",
    "# download link\n",
    "# - https://data.hartford.gov/datasets/hartfordgis::police-incidents-01012005-to-05182021/about\n",
    "data = pd.read_csv(\"./Police_Incidents_01012005_to_05182021.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Case_Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time_24HR</th>\n",
       "      <th>Address</th>\n",
       "      <th>UCR_1_Category</th>\n",
       "      <th>UCR_1_Description</th>\n",
       "      <th>UCR_1_Code</th>\n",
       "      <th>UCR_2_Category</th>\n",
       "      <th>UCR_2_Description</th>\n",
       "      <th>UCR_2_Code</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018388e+06</td>\n",
       "      <td>836860.062479</td>\n",
       "      <td>1</td>\n",
       "      <td>5001564</td>\n",
       "      <td>2005/01/11 00:00:00+00</td>\n",
       "      <td>1630</td>\n",
       "      <td>161 WASHINGTON ST</td>\n",
       "      <td>19* - CRIMES AGAINST THE PUBLIC</td>\n",
       "      <td>BREACH-PEACE</td>\n",
       "      <td>1901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FROG HOLLOW</td>\n",
       "      <td>CIRS-5001564-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.014889e+06</td>\n",
       "      <td>853267.562472</td>\n",
       "      <td>2</td>\n",
       "      <td>5001565</td>\n",
       "      <td>2005/01/11 00:00:00+00</td>\n",
       "      <td>1613</td>\n",
       "      <td>587 BLUE HILLS AV</td>\n",
       "      <td>51* - MISC. MANAGEMENT INFO.</td>\n",
       "      <td>COMM TENSION;COMM-SERVICE</td>\n",
       "      <td>5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>BLUE HILLS</td>\n",
       "      <td>CIRS-5001565-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.019379e+06</td>\n",
       "      <td>829663.750100</td>\n",
       "      <td>3</td>\n",
       "      <td>5001567</td>\n",
       "      <td>2005/01/10 00:00:00+00</td>\n",
       "      <td>2000</td>\n",
       "      <td>29 DOUGLAS ST</td>\n",
       "      <td>19* - CRIMES AGAINST THE PUBLIC</td>\n",
       "      <td>DOMESTIC</td>\n",
       "      <td>1904</td>\n",
       "      <td>19* - CRIMES AGAINST THE PUBLIC</td>\n",
       "      <td>DOMESTIC</td>\n",
       "      <td>1904</td>\n",
       "      <td>SOUTHEND</td>\n",
       "      <td>CIRS-5001567-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.017282e+06</td>\n",
       "      <td>838990.000148</td>\n",
       "      <td>4</td>\n",
       "      <td>5001569</td>\n",
       "      <td>2005/01/11 00:00:00+00</td>\n",
       "      <td>1645</td>\n",
       "      <td>BROAD ST &amp; CAPITOL AV</td>\n",
       "      <td>29* - FOUND PERSON/PROPERTY</td>\n",
       "      <td>M-V-S-O-T-R-L</td>\n",
       "      <td>2905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FROG HOLLOW</td>\n",
       "      <td>CIRS-5001569-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.014887e+06</td>\n",
       "      <td>839367.143423</td>\n",
       "      <td>5</td>\n",
       "      <td>5000476</td>\n",
       "      <td>2005/01/04 00:00:00+00</td>\n",
       "      <td>830</td>\n",
       "      <td>30 HAWTHORN ST</td>\n",
       "      <td>35* - MISC. CRIMES AGAINST PROPERTY</td>\n",
       "      <td>CR MISCHIEF 3</td>\n",
       "      <td>3503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>ASYLUM HILL</td>\n",
       "      <td>CIRS-5000476-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X              Y  OBJECTID  Case_Number                    Date  \\\n",
       "0  1.018388e+06  836860.062479         1      5001564  2005/01/11 00:00:00+00   \n",
       "1  1.014889e+06  853267.562472         2      5001565  2005/01/11 00:00:00+00   \n",
       "2  1.019379e+06  829663.750100         3      5001567  2005/01/10 00:00:00+00   \n",
       "3  1.017282e+06  838990.000148         4      5001569  2005/01/11 00:00:00+00   \n",
       "4  1.014887e+06  839367.143423         5      5000476  2005/01/04 00:00:00+00   \n",
       "\n",
       "   Time_24HR                Address                       UCR_1_Category  \\\n",
       "0       1630      161 WASHINGTON ST      19* - CRIMES AGAINST THE PUBLIC   \n",
       "1       1613      587 BLUE HILLS AV         51* - MISC. MANAGEMENT INFO.   \n",
       "2       2000          29 DOUGLAS ST      19* - CRIMES AGAINST THE PUBLIC   \n",
       "3       1645  BROAD ST & CAPITOL AV          29* - FOUND PERSON/PROPERTY   \n",
       "4        830         30 HAWTHORN ST  35* - MISC. CRIMES AGAINST PROPERTY   \n",
       "\n",
       "           UCR_1_Description  UCR_1_Code                   UCR_2_Category  \\\n",
       "0  BREACH-PEACE                     1901                              NaN   \n",
       "1  COMM TENSION;COMM-SERVICE        5104                              NaN   \n",
       "2  DOMESTIC                         1904  19* - CRIMES AGAINST THE PUBLIC   \n",
       "3  M-V-S-O-T-R-L                    2905                              NaN   \n",
       "4  CR MISCHIEF 3                    3503                              NaN   \n",
       "\n",
       "  UCR_2_Description  UCR_2_Code          Neighborhood         PRIMARY_KEY  \n",
       "0               NaN           0  FROG HOLLOW           CIRS-5001564-0      \n",
       "1               NaN           0  BLUE HILLS            CIRS-5001565-0      \n",
       "2          DOMESTIC        1904  SOUTHEND              CIRS-5001567-0      \n",
       "3               NaN           0  FROG HOLLOW           CIRS-5001569-0      \n",
       "4               NaN           0  ASYLUM HILL           CIRS-5000476-0      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90322, 15)\n"
     ]
    }
   ],
   "source": [
    "# if your machine can not hold the data, you can use the following code to select data in a specific time range\n",
    "data = data[(data[\"Date\"] >= \"2019-01-01\")]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data heuristically\n",
    "# we skip the data investigation step and directly give the conclusion and preprocess\n",
    "\n",
    "# 1. lowercase all columns\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "\n",
    "# 2. X(longitutde), Y(altitue) -> combine by s2cell as location feature\n",
    "S2_LEVEL = 10\n",
    "combine_xy = lambda x: s2cell.lat_lon_to_token(x[\"x\"], x[\"y\"], S2_LEVEL)\n",
    "data[\"location\"] = data[data[\"x\"].notna() & data[\"y\"].notna()].apply(combine_xy, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# 3. combine the Date and Time_24HR columns into a single datetime column\n",
    "candidates = zip(pd.to_datetime(data[\"date\"]).to_list(), data[\"time_24hr\"].to_list())\n",
    "datetime_values = []\n",
    "for (dts, hms) in candidates:\n",
    "    if hms < 60 :\n",
    "        hr = 0\n",
    "        m = hms\n",
    "    else:\n",
    "        hms_ = str(hms)\n",
    "        hr, m = int(hms_[:-2]), int(hms_[-2:])\n",
    "    datetime_values.append(datetime.fromtimestamp(dts.timestamp() + hr * 3600 + m * 60))\n",
    "data[\"datetime\"] = pd.to_datetime(datetime_values)\n",
    "\n",
    "\n",
    "# 4. remove the duplicated columns OBJECTID, Case_Number\n",
    "# these columns have high cardinality and are duplicated with PrimaryKey\n",
    "data = data.drop(columns=[\"objectid\", \"case_number\",\"date\", \"time_24hr\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column ucr_2_category has NaN values\n",
      "column ucr_2_description has NaN values\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].isna().any():\n",
    "        print(f\"column {column} has NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>address</th>\n",
       "      <th>ucr_1_category</th>\n",
       "      <th>ucr_1_description</th>\n",
       "      <th>ucr_1_code</th>\n",
       "      <th>ucr_2_category</th>\n",
       "      <th>ucr_2_description</th>\n",
       "      <th>ucr_2_code</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>primary_key</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>588689</th>\n",
       "      <td>1.018476e+06</td>\n",
       "      <td>836659.236765</td>\n",
       "      <td>172 WASHINGTON ST</td>\n",
       "      <td>06* - LARCENY</td>\n",
       "      <td>LARC4-MISCELL</td>\n",
       "      <td>685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SOUTH GREEN</td>\n",
       "      <td>CIRS-19037202-0</td>\n",
       "      <td>136fe1</td>\n",
       "      <td>2019-11-16 13:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588690</th>\n",
       "      <td>1.014989e+06</td>\n",
       "      <td>849540.060085</td>\n",
       "      <td>237 RIDGEFIELD ST</td>\n",
       "      <td>06* - LARCENY</td>\n",
       "      <td>LARC4-BUILDING</td>\n",
       "      <td>665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>BLUE HILLS</td>\n",
       "      <td>CIRS-19037207-0</td>\n",
       "      <td>344b5b</td>\n",
       "      <td>2019-11-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588691</th>\n",
       "      <td>1.014402e+06</td>\n",
       "      <td>850350.199869</td>\n",
       "      <td>351 BLUE HILLS AV</td>\n",
       "      <td>52* - SHOTS FIRED</td>\n",
       "      <td>SHOTS FIRED - UNCONFIRMED</td>\n",
       "      <td>5211</td>\n",
       "      <td>52* - SHOTS FIRED</td>\n",
       "      <td>SHOTS SPOTTER</td>\n",
       "      <td>5212</td>\n",
       "      <td>BLUE HILLS</td>\n",
       "      <td>CIRS-19037210-0</td>\n",
       "      <td>b05f05</td>\n",
       "      <td>2019-11-16 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588692</th>\n",
       "      <td>1.019147e+06</td>\n",
       "      <td>848978.169804</td>\n",
       "      <td>170 WESTLAND ST</td>\n",
       "      <td>05* - BURGLARY</td>\n",
       "      <td>BURG1-RES-DAY</td>\n",
       "      <td>502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NORTHEAST</td>\n",
       "      <td>CIRS-19037218-0</td>\n",
       "      <td>2f0535</td>\n",
       "      <td>2019-11-16 10:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588693</th>\n",
       "      <td>1.012601e+06</td>\n",
       "      <td>848372.560123</td>\n",
       "      <td>114 SHARON ST</td>\n",
       "      <td>52* - SHOTS FIRED</td>\n",
       "      <td>SHOTS FIRED - CONFIRMED</td>\n",
       "      <td>5210</td>\n",
       "      <td>52* - SHOTS FIRED</td>\n",
       "      <td>SHOTS SPOTTER</td>\n",
       "      <td>5212</td>\n",
       "      <td>BLUE HILLS</td>\n",
       "      <td>CIRS-19037232-0</td>\n",
       "      <td>a50c89</td>\n",
       "      <td>2019-11-16 19:24:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x              y            address     ucr_1_category  \\\n",
       "588689  1.018476e+06  836659.236765  172 WASHINGTON ST      06* - LARCENY   \n",
       "588690  1.014989e+06  849540.060085  237 RIDGEFIELD ST      06* - LARCENY   \n",
       "588691  1.014402e+06  850350.199869  351 BLUE HILLS AV  52* - SHOTS FIRED   \n",
       "588692  1.019147e+06  848978.169804    170 WESTLAND ST     05* - BURGLARY   \n",
       "588693  1.012601e+06  848372.560123      114 SHARON ST  52* - SHOTS FIRED   \n",
       "\n",
       "                ucr_1_description  ucr_1_code     ucr_2_category  \\\n",
       "588689  LARC4-MISCELL                     685                NaN   \n",
       "588690  LARC4-BUILDING                    665                NaN   \n",
       "588691  SHOTS FIRED - UNCONFIRMED        5211  52* - SHOTS FIRED   \n",
       "588692  BURG1-RES-DAY                     502                NaN   \n",
       "588693  SHOTS FIRED - CONFIRMED          5210  52* - SHOTS FIRED   \n",
       "\n",
       "       ucr_2_description  ucr_2_code          neighborhood  \\\n",
       "588689               NaN           0  SOUTH GREEN            \n",
       "588690               NaN           0  BLUE HILLS             \n",
       "588691     SHOTS SPOTTER        5212  BLUE HILLS             \n",
       "588692               NaN           0  NORTHEAST              \n",
       "588693     SHOTS SPOTTER        5212  BLUE HILLS             \n",
       "\n",
       "                primary_key location            datetime  \n",
       "588689  CIRS-19037202-0       136fe1 2019-11-16 13:16:00  \n",
       "588690  CIRS-19037207-0       344b5b 2019-11-14 00:00:00  \n",
       "588691  CIRS-19037210-0       b05f05 2019-11-16 15:00:00  \n",
       "588692  CIRS-19037218-0       2f0535 2019-11-16 10:45:00  \n",
       "588693  CIRS-19037232-0       a50c89 2019-11-16 19:24:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalizations, we inserted these data into the influxDB\n",
    "# InfluxDB data schema design like this:\n",
    "# - Bucket: Hartford\n",
    "# - Measurement: police_incidents\n",
    "# - Tags: \n",
    "#   - location: S2 cell token\n",
    "#   - neighborhood: neighborhood name\n",
    "#   - address: address\n",
    "#   - ucr_1_code\n",
    "#   - ucr_2_code\n",
    "#   - ucr_1_category\n",
    "#   - ucr_2_category\n",
    "#   - ucr_1_description\n",
    "#   - ucr_2_description\n",
    "# - Fields:\n",
    "#   - primary_key\n",
    "#   - lon\n",
    "#   - lat\n",
    "# - Time: datetime\n",
    "\n",
    "# In order to compare the difference of field and tag in query performance.\n",
    "# We create another bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket exist, delete it\n",
      "bucket Hartford created\n"
     ]
    }
   ],
   "source": [
    "BucketName = \"Hartford\"\n",
    "DEFAULT_ORG = \"docs\"\n",
    "bucket = client.buckets_api().find_bucket_by_name(bucket_name=BucketName)\n",
    "if bucket:\n",
    "    # bucket exist , reset it\n",
    "    print(\"bucket exist, delete it\")\n",
    "    client.buckets_api().delete_bucket(bucket)\n",
    "\n",
    "# create bucket\n",
    "bucket = client.buckets_api().create_bucket(bucket_name=BucketName, org_id=DEFAULT_ORG)\n",
    "if bucket:\n",
    "    print(f\"bucket {BucketName} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert these data into InfluxDB\n",
    "# [WARNING] before inserting through pd.Dataframe, we should guarantee there is no NaN value in the dataframe\n",
    "# OtherWise, the client will raise an error\n",
    "# @lingze: plz check above statement.\n",
    " \n",
    "# write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "# measurement = \"police_incidents\"\n",
    "# start = time.time()\n",
    "# batch_size = 2**15\n",
    "\n",
    "# for i in range(0, len(data), batch_size):\n",
    "#     data_batch = data.iloc[i:i+batch_size]\n",
    "#     write_api.write(\n",
    "#         bucket = BucketName,\n",
    "#         org = DEFAULT_ORG,\n",
    "#         record = data_batch,\n",
    "#         data_frame_measurement_name = measurement,\n",
    "#         data_frame_tag_columns = [\n",
    "#             \"location\", \n",
    "#             \"neighborhood\", \n",
    "#             \"address\", \n",
    "#             \"ucr_1_code\", \n",
    "#             \"ucr_2_code\", \n",
    "#             \"ucr_1_category\",\n",
    "#             \"ucr_2_category\", \n",
    "#             \"ucr_1_description\", \n",
    "#             \"ucr_2_description\"\n",
    "#         ],\n",
    "#         data_frame_field_columns = [\"primary_key\"],\n",
    "#         data_frame_time_index = \"datetime\"\n",
    "#     )\n",
    "#     print(f\"{i}/{len(data)} inserted\")\n",
    "# print(\"==> finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we divide the data into two parts, with NaN and w/o Nan\n",
    "nan_mask = data.isna().any(axis = 1)\n",
    "data_with_nan = data[nan_mask]\n",
    "data_without_nan = data[~nan_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "measurement = \"police_incidents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/46318 inserted\n",
      "32768/46318 inserted\n",
      "==> finished in 1.8253858089447021 seconds\n"
     ]
    }
   ],
   "source": [
    "# for data without Nan, we insert through DataFrame easily\n",
    "start = time.time()\n",
    "batch_size = 2**15\n",
    "for i in range(0, len(data_without_nan), batch_size):\n",
    "    batch_data = data_without_nan.iloc[i:i+batch_size]\n",
    "    write_api.write(\n",
    "            bucket = BucketName,\n",
    "            org = DEFAULT_ORG,\n",
    "            record = batch_data,\n",
    "            data_frame_measurement_name = measurement,\n",
    "            data_frame_tag_columns = [\n",
    "                \"location\", \n",
    "                \"neighborhood\", \n",
    "                \"address\", \n",
    "                \"ucr_1_code\", \n",
    "                \"ucr_2_code\", \n",
    "                \"ucr_1_category\",\n",
    "                \"ucr_2_category\", \n",
    "                \"ucr_1_description\", \n",
    "                \"ucr_2_description\"\n",
    "            ],\n",
    "            data_frame_field_columns = [\n",
    "                \"primary_key\",\n",
    "                \"x\",\n",
    "                \"y\"\n",
    "            ],\n",
    "            data_frame_timestamp_column = \"datetime\"\n",
    "        )\n",
    "    print(f\"{i}/{len(data_without_nan)} inserted\")\n",
    "end = time.time()\n",
    "print(f\"==> finished in {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data with Nan, we insert by constructing Point\n",
    "nan_mask = data_with_nan.isna()\n",
    "records = data_with_nan.to_dict(orient=\"records\")\n",
    "record_key_mask = data.isna().to_dict(orient=\"records\")\n",
    "\n",
    "tags_columns = [\n",
    "    \"location\", \n",
    "    \"neighborhood\", \n",
    "    \"address\", \n",
    "    \"ucr_1_code\", \n",
    "    \"ucr_2_code\", \n",
    "    \"ucr_1_category\",\n",
    "    \"ucr_2_category\", \n",
    "    \"ucr_1_description\", \n",
    "    \"ucr_2_description\"\n",
    "]\n",
    "\n",
    "fields_columns = [\n",
    "    \"primary_key\",\n",
    "    \"x\",\n",
    "    \"y\"\n",
    "]\n",
    "\n",
    "time_column = \"datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32767/44004 inserted\n",
      "44003/44004 inserted\n",
      "==> finished in 2.4716796875 seconds\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "batch_size = 2**15\n",
    "start = time.time()\n",
    "for idx,(record, key_mask) in enumerate(zip(records, record_key_mask)):\n",
    "    point_dict = {\n",
    "        \"measurement\": measurement\n",
    "    }\n",
    "    record_without_nan = {k: v for k, v in record.items() if not key_mask[k]}\n",
    "    point_dict['tags']={k: v for k, v in record_without_nan.items() if k in tags_columns}\n",
    "    point_dict['fields'] = {k: v for k, v in record_without_nan.items() if k in fields_columns}\n",
    "    point_dict['time'] = record_without_nan[time_column]\n",
    "    points.append(Point.from_dict(dictionary=point_dict))\n",
    "    \n",
    "    if (idx + 1) % batch_size == 0 or (idx + 1) == len(records):\n",
    "        write_api.write(bucket=BucketName, org=DEFAULT_ORG, record=points)\n",
    "        print(f\"{idx}/{len(records)} inserted\")\n",
    "        points = []\n",
    "\n",
    "end = time.time()\n",
    "print(f\"==> finished in {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_api = client.query_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> query finished in 25.178399801254272 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_values(['_result', 0, 7249, '1901', '19* - CRIMES AGAINST THE PUBLIC', 'BREACH-PEACE             ']),\n",
       " dict_values(['_result', 0, 4370, '5104', '51* - MISC. MANAGEMENT INFO.', 'COMM TENSION;COMM-SERVICE']),\n",
       " dict_values(['_result', 0, 3751, '3224', '32* - PROPERTY DAMAGE ACCIDENT', 'PROP DAM ACC             ']),\n",
       " dict_values(['_result', 0, 3657, '2903', '29* - FOUND PERSON/PROPERTY', 'ABANDONED M/V            ']),\n",
       " dict_values(['_result', 0, 3522, '3221', '32* - PROPERTY DAMAGE ACCIDENT', 'PROP DAM ACC             ']),\n",
       " dict_values(['_result', 0, 3334, '2090', '20* - RADIO SIGNAL', 'RADIO SIGNAL             ']),\n",
       " dict_values(['_result', 0, 3180, '3503', '35* - MISC. CRIMES AGAINST PROPERTY', 'CR MISCHIEF 3            ']),\n",
       " dict_values(['_result', 0, 3026, '2331', '23* - DRIVING LAWS', 'PARKING VIOLATION        ']),\n",
       " dict_values(['_result', 0, 2695, '5211', '52* - SHOTS FIRED', 'SHOTS FIRED - UNCONFIRMED']),\n",
       " dict_values(['_result', 0, 2395, '801', '08* - SIMPLE ASSAULT', 'ASSAULT 3                '])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Query\n",
    "# Question: Top-10 most common types of polices cases in all time\n",
    "query = \"\"\"\n",
    "import \"influxdata/influxdb/v1\"\n",
    "option v = {timeRangeStart:1970-01-01T00:00:00Z , timeRangeStop: now()}\n",
    "\n",
    "from(bucket: \"Hartford\")\n",
    "    |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"police_incidents\" and r[\"_field\"] == \"primary_key\")\n",
    "    |> group(columns: [\"ucr_1_code\", \"ucr_1_category\", \"ucr_1_description\"])\n",
    "    |> count(column: \"_value\") // count the number of records per group\n",
    "    |> group() // Ungroup to allow sorting across all groups\n",
    "    |> sort(columns: [\"_value\"], desc: true) // Sort by count in descending order\n",
    "    |> limit(n:10)\n",
    "    |> keep(columns: [\"ucr_1_code\", \"_value\", \"ucr_1_category\", \"ucr_1_description\"]) // Specify columns to retain\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tables = query_api.query(query=query, org=DEFAULT_ORG)\n",
    "end = time.time()\n",
    "print(f\"==> query finished in {end - start} seconds\")\n",
    "tables.to_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> query finished in 3.6595773696899414 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_values(['_result', 0, datetime.datetime(2018, 12, 27, 0, 0, tzinfo=tzlocal()), 14]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 1, 3, 0, 0, tzinfo=tzlocal()), 36]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 1, 10, 0, 0, tzinfo=tzlocal()), 42]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 1, 17, 0, 0, tzinfo=tzlocal()), 46]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 1, 24, 0, 0, tzinfo=tzlocal()), 39]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 1, 31, 0, 0, tzinfo=tzlocal()), 48]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 2, 7, 0, 0, tzinfo=tzlocal()), 52]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 2, 14, 0, 0, tzinfo=tzlocal()), 40]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 2, 21, 0, 0, tzinfo=tzlocal()), 47]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 2, 28, 0, 0, tzinfo=tzlocal()), 36]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 3, 7, 0, 0, tzinfo=tzlocal()), 48]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 3, 14, 0, 0, tzinfo=tzlocal()), 39]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 3, 21, 0, 0, tzinfo=tzlocal()), 44]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 3, 28, 0, 0, tzinfo=tzlocal()), 49]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 4, 4, 0, 0, tzinfo=tzlocal()), 45]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 4, 11, 0, 0, tzinfo=tzlocal()), 47]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 4, 18, 0, 0, tzinfo=tzlocal()), 51]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 4, 25, 0, 0, tzinfo=tzlocal()), 44]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 5, 2, 0, 0, tzinfo=tzlocal()), 53]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 5, 9, 0, 0, tzinfo=tzlocal()), 43]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 5, 16, 0, 0, tzinfo=tzlocal()), 62]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 5, 23, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 5, 30, 0, 0, tzinfo=tzlocal()), 51]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 6, 6, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 6, 13, 0, 0, tzinfo=tzlocal()), 54]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 6, 20, 0, 0, tzinfo=tzlocal()), 41]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 6, 27, 0, 0, tzinfo=tzlocal()), 47]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 7, 4, 0, 0, tzinfo=tzlocal()), 61]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 7, 11, 0, 0, tzinfo=tzlocal()), 43]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 7, 18, 0, 0, tzinfo=tzlocal()), 57]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 7, 25, 0, 0, tzinfo=tzlocal()), 65]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 8, 1, 0, 0, tzinfo=tzlocal()), 50]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 8, 8, 0, 0, tzinfo=tzlocal()), 48]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 8, 15, 0, 0, tzinfo=tzlocal()), 52]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 8, 22, 0, 0, tzinfo=tzlocal()), 42]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 8, 29, 0, 0, tzinfo=tzlocal()), 72]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 9, 5, 0, 0, tzinfo=tzlocal()), 64]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 9, 12, 0, 0, tzinfo=tzlocal()), 61]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 9, 19, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 9, 26, 0, 0, tzinfo=tzlocal()), 72]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 10, 3, 0, 0, tzinfo=tzlocal()), 50]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 10, 10, 0, 0, tzinfo=tzlocal()), 48]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 10, 17, 0, 0, tzinfo=tzlocal()), 44]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 10, 24, 0, 0, tzinfo=tzlocal()), 65]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 10, 31, 0, 0, tzinfo=tzlocal()), 64]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 11, 7, 0, 0, tzinfo=tzlocal()), 44]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 11, 14, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 11, 21, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 11, 28, 0, 0, tzinfo=tzlocal()), 57]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 12, 5, 0, 0, tzinfo=tzlocal()), 74]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 12, 12, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 12, 19, 0, 0, tzinfo=tzlocal()), 69]),\n",
       " dict_values(['_result', 0, datetime.datetime(2019, 12, 26, 0, 0, tzinfo=tzlocal()), 67]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 1, 2, 0, 0, tzinfo=tzlocal()), 81]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 1, 9, 0, 0, tzinfo=tzlocal()), 71]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 1, 16, 0, 0, tzinfo=tzlocal()), 79]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 1, 23, 0, 0, tzinfo=tzlocal()), 73]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 1, 30, 0, 0, tzinfo=tzlocal()), 55]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 2, 6, 0, 0, tzinfo=tzlocal()), 51]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 2, 13, 0, 0, tzinfo=tzlocal()), 50]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 2, 20, 0, 0, tzinfo=tzlocal()), 55]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 2, 27, 0, 0, tzinfo=tzlocal()), 41]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 3, 5, 0, 0, tzinfo=tzlocal()), 67]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 3, 12, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 3, 19, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 3, 26, 0, 0, tzinfo=tzlocal()), 59]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 4, 2, 0, 0, tzinfo=tzlocal()), 62]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 4, 9, 0, 0, tzinfo=tzlocal()), 54]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 4, 16, 0, 0, tzinfo=tzlocal()), 46]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 4, 23, 0, 0, tzinfo=tzlocal()), 49]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 4, 30, 0, 0, tzinfo=tzlocal()), 72]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 5, 7, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 5, 14, 0, 0, tzinfo=tzlocal()), 67]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 5, 21, 0, 0, tzinfo=tzlocal()), 64]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 5, 28, 0, 0, tzinfo=tzlocal()), 80]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 6, 4, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 6, 11, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 6, 18, 0, 0, tzinfo=tzlocal()), 84]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 6, 25, 0, 0, tzinfo=tzlocal()), 75]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 7, 2, 0, 0, tzinfo=tzlocal()), 75]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 7, 9, 0, 0, tzinfo=tzlocal()), 50]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 7, 16, 0, 0, tzinfo=tzlocal()), 78]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 7, 23, 0, 0, tzinfo=tzlocal()), 47]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 7, 30, 0, 0, tzinfo=tzlocal()), 70]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 8, 6, 0, 0, tzinfo=tzlocal()), 80]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 8, 13, 0, 0, tzinfo=tzlocal()), 71]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 8, 20, 0, 0, tzinfo=tzlocal()), 82]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 8, 27, 0, 0, tzinfo=tzlocal()), 65]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 9, 3, 0, 0, tzinfo=tzlocal()), 74]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 9, 10, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 9, 17, 0, 0, tzinfo=tzlocal()), 59]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 9, 24, 0, 0, tzinfo=tzlocal()), 71]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 10, 1, 0, 0, tzinfo=tzlocal()), 87]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 10, 8, 0, 0, tzinfo=tzlocal()), 74]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 10, 15, 0, 0, tzinfo=tzlocal()), 72]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 10, 22, 0, 0, tzinfo=tzlocal()), 80]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 10, 29, 0, 0, tzinfo=tzlocal()), 80]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 11, 5, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 11, 12, 0, 0, tzinfo=tzlocal()), 53]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 11, 19, 0, 0, tzinfo=tzlocal()), 55]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 11, 26, 0, 0, tzinfo=tzlocal()), 65]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 12, 3, 0, 0, tzinfo=tzlocal()), 64]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 12, 10, 0, 0, tzinfo=tzlocal()), 57]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 12, 17, 0, 0, tzinfo=tzlocal()), 50]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 12, 24, 0, 0, tzinfo=tzlocal()), 69]),\n",
       " dict_values(['_result', 0, datetime.datetime(2020, 12, 31, 0, 0, tzinfo=tzlocal()), 45]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 1, 7, 0, 0, tzinfo=tzlocal()), 49]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 1, 14, 0, 0, tzinfo=tzlocal()), 59]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 1, 21, 0, 0, tzinfo=tzlocal()), 53]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 1, 28, 0, 0, tzinfo=tzlocal()), 67]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 2, 4, 0, 0, tzinfo=tzlocal()), 63]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 2, 11, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 2, 18, 0, 0, tzinfo=tzlocal()), 61]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 2, 25, 0, 0, tzinfo=tzlocal()), 53]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 3, 4, 0, 0, tzinfo=tzlocal()), 48]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 3, 11, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 3, 18, 0, 0, tzinfo=tzlocal()), 60]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 3, 25, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 4, 1, 0, 0, tzinfo=tzlocal()), 61]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 4, 8, 0, 0, tzinfo=tzlocal()), 54]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 4, 15, 0, 0, tzinfo=tzlocal()), 56]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 4, 22, 0, 0, tzinfo=tzlocal()), 40]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 4, 29, 0, 0, tzinfo=tzlocal()), 69]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 5, 6, 0, 0, tzinfo=tzlocal()), 59]),\n",
       " dict_values(['_result', 0, datetime.datetime(2021, 5, 13, 0, 0, tzinfo=tzlocal()), 47])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Query\n",
    "# Question: Number of cases with code \"1901\" over time grouped by week\n",
    "query = \"\"\"\n",
    "import \"influxdata/influxdb/v1\"\n",
    "option v = {timeRangeStart:1970-01-01T00:00:00Z , timeRangeStop: now()}\n",
    "\n",
    "from(bucket: \"Hartford\")\n",
    "    |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"police_incidents\" and r[\"_field\"] == \"primary_key\")\n",
    "    |> filter(fn: (r) => r[\"ucr_1_code\"] == \"1901\")\n",
    "    |> truncateTimeColumn(unit: 1w) // Truncate time to week\n",
    "    |> group(columns: [\"_time\"])\n",
    "    |> count(column: \"_value\")\n",
    "    |> group()\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tables = query_api.query(query=query, org=DEFAULT_ORG)\n",
    "end = time.time()\n",
    "print(f\"==> query finished in {end - start} seconds\")\n",
    "tables.to_values()\n",
    "# better to execute above result in the InfluxDB UI\n",
    "# it will visualize the result in timeline graph, more intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> query finished in 26.107426166534424 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[dict_values(['Latest Incident by ucr_1_code', 0, datetime.datetime(2021, 5, 17, 2, 31, tzinfo=tzlocal()), 'CIRS-21014480-0    ', '60 CAMPFIELD AV', '19* - CRIMES AGAINST THE PUBLIC', '1901', 'BREACH-PEACE             ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 1, datetime.datetime(2021, 5, 16, 17, 45, tzinfo=tzlocal()), 'CIRS-21014436-0    ', '24 MERRILL ST', '20* - RADIO SIGNAL', '2090', 'RADIO SIGNAL             ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 2, datetime.datetime(2021, 5, 14, 13, 33, tzinfo=tzlocal()), 'CIRS-21014211-0    ', '203 TRUMBULL ST', '23* - DRIVING LAWS', '2331', 'PARKING VIOLATION        ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 3, datetime.datetime(2021, 5, 12, 17, 50, tzinfo=tzlocal()), 'CIRS-21014004-0    ', '69 CURTISS ST', '29* - FOUND PERSON/PROPERTY', '2903', 'ABANDONED M/V            ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 4, datetime.datetime(2021, 5, 14, 20, 15, tzinfo=tzlocal()), 'CIRS-21014250-0    ', 'COGSWELL ST & GARDEN ST', '32* - PROPERTY DAMAGE ACCIDENT', '3221', 'PROP DAM ACC             ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 5, datetime.datetime(2021, 5, 13, 13, 50, tzinfo=tzlocal()), 'CIRS-21014107-0    ', '44 EATON ST', '32* - PROPERTY DAMAGE ACCIDENT', '3224', 'PROP DAM ACC             ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 6, datetime.datetime(2021, 5, 14, 9, 7, tzinfo=tzlocal()), 'CIRS-21014192-0    ', '38 LAWRENCE ST', '35* - MISC. CRIMES AGAINST PROPERTY', '3503', 'CR MISCHIEF 3            ']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 7, datetime.datetime(2021, 5, 15, 12, 17, tzinfo=tzlocal()), 'CIRS-21014305-0    ', '50 HAMILTON ST', '51* - MISC. MANAGEMENT INFO.', '5104', 'COMM TENSION;COMM-SERVICE']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 8, datetime.datetime(2021, 5, 16, 22, 43, tzinfo=tzlocal()), 'CIRS-21014461-0    ', '264 CAPEN ST', '52* - SHOTS FIRED', '5211', 'SHOTS FIRED - UNCONFIRMED']),\n",
       " dict_values(['Latest Incident by ucr_1_code', 9, datetime.datetime(2021, 5, 15, 20, 24, tzinfo=tzlocal()), 'CIRS-21014343-0    ', '194 WASHINGTON ST', '08* - SIMPLE ASSAULT', '801', 'ASSAULT 3                '])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third Query\n",
    "\n",
    "# Question: the latest incident (most recent time) for the top ten ucr_1_code from police_incidents measurement \n",
    "\n",
    "# we need to utilize the result of Query 1\n",
    "query = \"\"\"\n",
    "import \"influxdata/influxdb/v1\"\n",
    "option v = {timeRangeStart:1970-01-01T00:00:00Z , timeRangeStop: now()}\n",
    "codeset = [\"1901\", \"5104\", \"3224\", \"2903\", \"3221\", \"2090\", \"3503\", \"2331\", \"5211\", \"801\"]\n",
    "from(bucket: \"Hartford\")\n",
    "    |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n",
    "    |> filter(fn: (r) => r[\"_measurement\"] == \"police_incidents\")\n",
    "    |> filter(fn: (r) => r[\"_field\"] == \"primary_key\")\n",
    "    |> filter(fn: (r) => contains(value: r.ucr_1_code, set:codeset) ) \n",
    "    |> group(columns: [\"ucr_1_code\"])  // Group by ucr_1_code\n",
    "    |> sort(columns: [\"_time\"], desc: true)  // Sort by time, most recent first\n",
    "    |> limit(n: 10)  // Limit to the top 10 most recent incidents\n",
    "    |> last()  // Get the most recent incident for each ucr_1_code\n",
    "    |> keep(columns: [\"ucr_1_code\", \"_value\", \"ucr_1_category\", \"ucr_1_description\", \"_time\", \"address\"])\n",
    "    |> yield(name: \"Latest Incident by ucr_1_code\")\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tables = query_api.query(query=query, org=DEFAULT_ORG)\n",
    "end = time.time()\n",
    "print(f\"==> query finished in {end - start} seconds\")\n",
    "tables.to_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean: 1017189.1215648074, y_mean: 839065.748621916, x_std: 3500.0514335853445, y_std: 6721.366808089502, radius: 7578.069134458335\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json; charset=utf-8', 'Vary': 'Accept-Encoding', 'X-Influxdb-Build': 'OSS', 'X-Influxdb-Version': 'v2.7.11', 'X-Platform-Error-Code': 'not found', 'Date': 'Thu, 09 Jan 2025 14:36:09 GMT', 'Transfer-Encoding': 'chunked'})\nHTTP response body: b'{\"code\":\"not found\",\"message\":\"error calling function \\\\\"filterRows\\\\\" @11:8-11:88: error calling function \\\\\"gridFilter\\\\\" @experimental/geo/geo.flux|865:24-871:22: error calling function \\\\\"tableFind\\\\\" @experimental/geo/geo.flux|505:16-505:61: no table found\"}'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 29\u001b[0m\n\u001b[1;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mimport \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfluxdata/influxdb/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mimport \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental/geo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    |> group()\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 29\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[43mquery_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_ORG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> query finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/client/query_api.py:203\u001b[0m, in \u001b[0;36mQueryApi.query\u001b[0;34m(self, query, org, params)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute synchronous Flux query and return result as a :class:`~influxdb_client.client.flux_table.FluxTable` list.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m:param query: the Flux query\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    ]\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    201\u001b[0m org \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_org_param(org)\n\u001b[0;32m--> 203\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_dialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_tables(response, query_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_query_options())\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/service/query_service.py:285\u001b[0m, in \u001b[0;36mQueryService.post_query\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_query_with_http_info(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     (data) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_query_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/service/query_service.py:311\u001b[0m, in \u001b[0;36mQueryService.post_query_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Query data.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mRetrieves data from buckets.  Use this endpoint to send a Flux query request and retrieve data from a bucket.  #### Rate limits (with InfluxDB Cloud)  `read` rate limits apply. For more information, see [limits and adjustable quotas](https://docs.influxdata.com/influxdb/cloud/account-management/limits/).  #### Related guides  - [Query with the InfluxDB API](https://docs.influxdata.com/influxdb/latest/query-data/execute-queries/influx-api/) - [Get started with Flux](https://docs.influxdata.com/flux/v0.x/get-started/)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m         returns the request thread.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    308\u001b[0m local_var_params, path_params, query_params, header_params, body_params \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_query_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/v2/query\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlopen_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murlopen_kw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/_sync/api_client.py:343\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, urlopen_kw)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make the HTTP request (synchronous) and Return deserialized data.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murlopen_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    351\u001b[0m                                    method, path_params, query_params,\n\u001b[1;32m    352\u001b[0m                                    header_params, body,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m                                    collection_formats,\n\u001b[1;32m    357\u001b[0m                                    _preload_content, _request_timeout, urlopen_kw))\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/_sync/api_client.py:173\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, urlopen_kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m urlopen_kw \u001b[38;5;241m=\u001b[39m urlopen_kw \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    181\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/_sync/api_client.py:388\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout, **urlopen_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(url,\n\u001b[1;32m    380\u001b[0m                                     query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    381\u001b[0m                                     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m                                     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    386\u001b[0m                                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(url,\n\u001b[1;32m    398\u001b[0m                                 query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    399\u001b[0m                                 headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m                                 body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    404\u001b[0m                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/_sync/rest.py:311\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout, **urlopen_kw)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, post_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m          body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw):\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform POST HTTP request.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4221/lib/python3.9/site-packages/influxdb_client/_sync/rest.py:261\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout, **urlopen_kw)\u001b[0m\n\u001b[1;32m    258\u001b[0m     _BaseRESTClient\u001b[38;5;241m.\u001b[39mlog_body(r\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<<<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json; charset=utf-8', 'Vary': 'Accept-Encoding', 'X-Influxdb-Build': 'OSS', 'X-Influxdb-Version': 'v2.7.11', 'X-Platform-Error-Code': 'not found', 'Date': 'Thu, 09 Jan 2025 14:36:09 GMT', 'Transfer-Encoding': 'chunked'})\nHTTP response body: b'{\"code\":\"not found\",\"message\":\"error calling function \\\\\"filterRows\\\\\" @11:8-11:88: error calling function \\\\\"gridFilter\\\\\" @experimental/geo/geo.flux|865:24-871:22: error calling function \\\\\"tableFind\\\\\" @experimental/geo/geo.flux|505:16-505:61: no table found\"}'\n"
     ]
    }
   ],
   "source": [
    "# Fourth Query\n",
    "\n",
    "# Question: Number of crime cases within a radius of Hartford, CT\n",
    "\n",
    "# first, we can get the HartFord CI coordinates is (lat, lon) = (X, Y) = (30.04, 31.23)\n",
    "# 30 miles is 48.2803 km\n",
    "\n",
    "x_mean = data[\"x\"].mean()\n",
    "y_mean = data[\"y\"].mean()\n",
    "x_std = data[\"x\"].std()\n",
    "y_std = data[\"y\"].std()\n",
    "radius = (x_std**2 + y_std**2)**(1/2)\n",
    "print(f\"x_mean: {x_mean}, y_mean: {y_mean}, x_std: {x_std}, y_std: {y_std}, radius: {radius}\")\n",
    "\n",
    "query = \"\"\"\n",
    "import \"influxdata/influxdb/v1\"\n",
    "import \"experimental/geo\"\n",
    "\n",
    "option v = {timeRangeStart:1970-01-01T00:00:00Z , timeRangeStop: now()}\n",
    "\n",
    "from(bucket: \"Hartford\")\n",
    "    |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n",
    "    |> filter(fn: (r) => r._measurement == \"policeincidents\" and r._field == \"x\" or r.field == \"y\")\n",
    "    |> geo.shapeData(latField: \"x\", lonField: \"y\", level:10)\n",
    "    |> geo.filterRows(region: {lat: 1017189, lon: 839065, radius: 7500}, strict: false)  \n",
    "    |> group()\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "tables = query_api.query(query=query, org=DEFAULT_ORG)\n",
    "end = time.time()\n",
    "print(f\"==> query finished in {end - start} seconds\")\n",
    "tables.to_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
